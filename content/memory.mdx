---
title: "Memory"
description: "Reference documentation for memory models bundled with Tiles"
---

# Memory

Our approach to model selection is to use the best model for each task within the constraints of the supported hardware. Currently, Tiles is designed to cover everyday tasks, with plans to expand into more domain-specific use cases over time. We provide a carefully tested combination of system prompts, tools, and models, so users do not need to handle model selection or scaffolding themselves.

## Insiders

This memory setup is part of the Tiles Insiders channel and is experimental in nature.

Tiles bundles a fine-tuned model to manage context and memories locally on-device using hyperlinked Markdown files. Currently, we use the [mem-agent](https://huggingface.co/driaforall/mem-agent) model from [Dria](https://dria.co/), based on `qwen3-4B-thinking-2507`, and are in the process of training our initial in-house memory models. Read more about their training process and scaffolding design in their paper [here](https://huggingface.co/blog/driaforall/mem-agent-blog).

These models use a human-readable external memory stored as Markdown, along with learned policies trained via reinforcement learning on synthetically generated data. These policies decide when to call Python functions that retrieve, update, or clarify memory, allowing the assistant to maintain and refine persistent knowledge across sessions.

### Implementation

The agent works with Obsidian-like directories of Markdown files as its knowledge base. These files are both human-readable and hyperlinked. The core protocol is defined in a single system prompt. The source is available [here](https://github.com/firstbatchxyz/mem-agent-mcp/blob/main/agent/system_prompt.txt).

Dria trained a fine-tuned model so the agent strictly adheres to the enforced prompt format.

### Response Structure

The model's response is split into three structured sections:

- `<think>`: reasoning  
- `<python>`: executable code that calls predefined functions  
- `<reply>`: final output after memory interaction  

### Execution

The Python block is executed in a sandbox, and its results are returned as `<result>` tags. This is how the agent forms its reasoning to action loop.

import Image from 'next/image'

<figure style={{ maxWidth: '420px', margin: '1.5rem 0' }}>
  <Image src="/loop.jpg" alt="Reasoning to action loop" width={420} height={315} style={{ borderRadius: '8px', border: '1px solid var(--nextra-border-color, rgba(0,0,0,0.1))', width: '100%', height: 'auto' }} />
</figure>

Tiles executes the generated Python code in a sandbox. The code calls tools such as `create_file`, `update_file`, and others mentioned above. These tools are plain Python functions.

<figure style={{ maxWidth: '420px', margin: '1.5rem 0' }}>
  <Image src="/python.jpg" alt="Python tools" width={420} height={315} style={{ borderRadius: '8px', border: '1px solid var(--nextra-border-color, rgba(0,0,0,0.1))', width: '100%', height: 'auto' }} />
</figure>
