---
title: "Resources"
ogTitle: "Resources"
description: "A living index of resources that inform and inspire our work."
type: "website"
---

# Resources

Below is a living index of resources that inform and inspire our work.

## Motivations

- ✨ [This is Cuba's Netflix, Hulu, and Spotify – all without the internet](https://www.youtube.com/watch?v=fTTno8D-b2E)
- [Cuba's Underground Gaming Network](https://www.youtube.com/watch?v=lEplzHraw3c)
- ✨ [Weathering Software Winter](https://www.youtube.com/watch?v=9TJuOwy4aGA&t=968s)
- ✨ [The Memory Walled Garden](https://asimovaddendum.substack.com/p/the-memory-walled-garden)
- ✨ [The Resonant Computing Manifesto](https://resonantcomputing.org)
- ✨ [Why Tech Needs Personalization](https://om.co/2025/10/29/why-tech-needs-personalization/)
- ✨ [File over app](https://stephango.com/file-over-app)

## Research

- ✨ [Intelligence Per Watt: A Study of Local Intelligence Efficiency](https://hazyresearch.stanford.edu/blog/2025-11-11-ipw)
- [The Bitter Lesson of LLM Extensions](https://www.sawyerhood.com/blog/llm-extension)
- ✨ [The Continual Learning Problem, Jessy Lin](https://jessylin.com/2025/10/20/continual-learning/)
- ✨ [mem-agent: Equipping LLM Agents with Memory Using RL](https://dria.co/research/mem-agent:-equipping-llm-agents-with-memory-using-rl)
- ✨ [Xet is on the Hub](https://huggingface.co/blog/xet-on-the-hub)
- ✨ [MIR, Machine Intelligence Resource, A naming schema for AIGC/ML work, Darkshapes](https://huggingface.co/darkshapes/MIR_)
- ✨ [Jan-nano Technical Report](https://arxiv.org/abs/2506.22760)
-  [Minions: the rise of small, on-device LMs](https://hazyresearch.stanford.edu/blog/2025-02-24-minions)
- ✨ [Memory Layers at Scale, Meta FAIR](https://arxiv.org/abs/2412.09764)
- [On the Way to LLM Personalization: Learning to Remember User Conversations, Apple Machine Learning Research](https://machinelearning.apple.com/research/on-the-way)
- [Memory OS of AI Agent](https://arxiv.org/abs/2506.06326)
- ✨ [Self-driving infrastructure](https://vercel.com/blog/self-driving-infrastructure)
- ✨ [LoRA Without Regret](https://thinkingmachines.ai/blog/lora/)
- ✨ [A Preliminary Report On Edge-Verified Machine Learning (evML)](https://github.com/exo-explore/evML/blob/main/A_Preliminary_Report_On_evML.pdf)
- ✨ [Pretraining with hierarchical memories: separating long-tail and common knowledge, Apple](https://www.arxiv.org/abs/2510.02375)
- [Titans + MIRAS: Helping AI have long-term memory, Google Research](https://research.google/blog/titans-miras-helping-ai-have-long-term-memory/)
- [LoRA Learns Less and Forgets Less](https://arxiv.org/abs/2405.09673)
- ✨ [The Bitter Lesson is coming for Tokenization](https://lucalp.dev/bitter-lesson-tokenization-and-blt/)
- ✨ [Text-to-LoRA: Hypernetworks that adapt LLMs for specific benchmark tasks using only textual task description as the input, Sakana AI](https://arxiv.org/abs/2506.06105)
- [Transformer²: Self-Adaptive LLMs](https://sakana.ai/transformer-squared/)
- [How memory augmentation can improve large language models, IBM Research](https://research.ibm.com/blog/memory-augmented-LLMs)
- [Mixture-of-Depths: Dynamically allocating compute in transformer-based language models](https://arxiv.org/abs/2404.02258)
- ✨ [The Power of Efficiency: Edge AI's Role in Sustainable Generative AI Adoption](https://creativestrategies.com/research/gen-ai-edge-testing/)
- ✨ [Small Language Models are the Future of Agentic AI, NVIDIA Research](https://arxiv.org/abs/2506.02153)
- ✨ [Defeating Prompt Injections by Design, Google Deepmind](https://arxiv.org/abs/2503.18813)
- [LLM in a flash: Efficient Large Language Model Inference with Limited Memory](https://arxiv.org/abs/2312.11514)
- [Introducing FlexOlmo: a new paradigm for language model training and data collaboration, Allen AI](https://allenai.org/blog/flexolmo)
- [WhisperKit: On-device Real-time ASR with Billion-Scale Transformers, Argmax](https://openreview.net/attachment?id=6lC3MPFbVg&name=pdf)
- ✨ [Towards Large-scale Training on Apple Silicon, Exo Labs](https://openreview.net/pdf?id=TJjP8d5bms)
- [Kinetics: Rethinking Test-Time Scaling Laws](https://openreview.net/attachment?id=qxnJrm47Ag&name=pdf)
- [Enhancing Reasoning Capabilities of Small Language Models with Blueprints and Prompt Template Search](https://openreview.net/attachment?id=LsNstclw8Z&name=pdf)
- [LoFT: Low-Rank Adaptation That Behaves Like Full Fine-Tuning](https://arxiv.org/pdf/2505.21289)
- [AirLLM: Diffusion Policy-based Adaptive LoRA for Remote Fine-Tuning of LLM over the Air](https://arxiv.org/abs/2507.11515)
- [Comparative Analysis of Retrieval Systems in the Real World](https://arxiv.org/pdf/2405.02048)
- [FedVLM: Scalable Personalized Vision-Language Models through Federated Learning](https://arxiv.org/abs/2507.17088)
- [On the Way to LLM Personalization: Learning to Remember User Conversations](https://arxiv.org/abs/2411.13405)
- [A Preliminary Report On Edge-Verified Machine Learning, Exo Labs](https://github.com/exo-explore/evML/blob/main/A_Preliminary_Report_On_evML.pdf)
- ✨ [Model Merging in LLMs, MLLMs, and Beyond: Methods, Theories, Applications and Opportunities](https://arxiv.org/abs/2408.07666)
- ✨ [Intent-Based Architecture and Their Risks](https://www.paradigm.xyz/2023/06/intents)
- [Your LLM Knows the Future: Uncovering Its Multi-Token Prediction Potential](https://arxiv.org/abs/2507.11851)
- [Cephalo: Multi-Modal Vision-Language Models for Bio-Inspired Materials Analysis and Design](https://arxiv.org/abs/2405.19076)
- [Towards Feasible Private Distributed LLM Inference, Dria](https://dria.co/research/towards-feasible-private-distributed-llm-inference)
- ✨ [ChatGPT Memory and the Bitter Lesson](https://www.shloked.com/writing/chatgpt-memory-bitter-lesson)
- ✨ [OpenPoke: Recreating Poke's Architecture](https://www.shloked.com/writing/openpoke)
- [Anthropic's Opinionated Memory Bet](https://shloked.com/writing/claude-memory-tool)
- [Mind the Trust Gap: Fast, Private Local-to-Cloud LLM Chat](https://hazyresearch.stanford.edu/blog/2025-05-12-security)
- [MCP Colors: Systematically deal with prompt injection risk](https://timkellogg.me/blog/2025/11/03/colors)
- [New physical attacks are quickly diluting secure enclave defenses from Nvidia, AMD, and Intel](https://arstechnica.com/security/2025/10/new-physical-attacks-are-quickly-diluting-secure-enclave-defenses-from-nvidia-amd-and-intel/)
- [Benchmarking Honcho](https://blog.plasticlabs.ai/research/Benchmarking-Honcho)
- [Git Based Memory Storage for Conversational AI Agent](https://github.com/Growth-Kinetics/DiffMem)

## Reference 

- [Ellora: Enhancing LLMs with LoRA](https://github.com/codelion/ellora_)
- ✨ [Understanding AI/LLM Quantisation Through Interactive Visualisations](https://smcleod.net/2024/07/understanding-ai/llm-quantisation-through-interactive-visualisations/)
- ✨ [User-Agent header, MDN](https://developer.mozilla.org/en-US/docs/Web/HTTP/Reference/Headers/User-Agent)
- ✨ [Modelfile Reference, Ollama Documentation](https://docs.ollama.com/modelfile)
- [Dria Inference Arena – Compare Benchmarks Across LLMs, Inference Engines & Hardware](https://dria.co/inference-arena)
- [The GPU-Poor LLM Gladiator Arena](https://huggingface.co/spaces/k-mktr/gpu-poor-llm-arena)
- ✨ [The Kaitchup Index: A Leaderboard for Quantized LLMs](https://kaitchup.substack.com/p/the-kaitchup-index)
- [InferenceMAX™: Open Source Inference Benchmarking](https://newsletter.semianalysis.com/p/inferencemax-open-source-inference)
- [RFT, DPO, SFT: Fine-tuning with OpenAI — Ilan Bigio, OpenAI](https://www.youtube.com/watch?v=JfaLQqfXqPA)
- ✨ [Hand-picked selection of articles on AI fundamentals/concepts that cover the entire process of building neural nets to training them to evaluating results.](https://aman.ai/primers/ai/)
- ✨ [The State of Open Models](https://youtu.be/FUcilE5Gx_0?si=9aP_NAzTKi8qw78n)
- ✨ [The State of On-Device LLMs](https://app.getcontrast.io/register/sota-the-state-of-llms)
- [How to Scale Your Model](https://jax-ml.github.io/scaling-book/)
- ✨ [r/LocalLLaMA](https://www.reddit.com/r/LocalLLaMA/)
- ✨ [An Analogy for Understanding Transformers](https://www.lesswrong.com/posts/euam65XjigaCJQkcN/an-analogy-for-understanding-transformers)
- ✨ [Neural networks, 3Blue1Brown](https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi)
- [GGUF Quantization Docs (Unofficial)](https://github.com/iuliaturc/gguf-docs)
- [Reverse-engineering GGUF | Post-Training Quantization](https://www.youtube.com/watch?v=vW30o4U9BFE)
- [Choosing a GGUF Model: K-Quants, I-Quants, and Legacy Formats](https://kaitchup.substack.com/p/choosing-a-gguf-model-k-quants-i)
- [Reference implementation of the Transformer architecture optimized for Apple Neural Engine](https://github.com/apple/ml-ane-transformers)
- ✨ [LLMs on a Budget](https://benjaminmarie.gumroad.com/l/llms-on-a-budget)
- ✨ [Personalized Machine Learning](https://cseweb.ucsd.edu/~jmcauley/pml/)

